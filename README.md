# Kafka Playground #

## Overview ##
This has a couple of Java projects that I used to learn how to run and interact with Kafka. They're really simple examples,
but I think they should get you started.

> Note about versions: I built and tested these using the [Azul Zulu](https://www.azul.com/downloads/?package=jdk) build of OpenJDK 11.  

## Installation ##
In the docker directory, there is a docker-compose.yaml file that can be used to create a basic Kafka setup
It creates these containers:
* **zookeeper** - This is required by Kafka to manage one or more brokers
* **broker** - This is the main Kafka broker.  
You refer to it in Kafka configurations and commandline calls as the bootstrap server with the address: localhost:9092 (Assuming that you have installed it locally)
* **schema-registry** - This will store the ProtoBuf schemas that you need Kafka to use to encode/decode messages
* **ui** - This is a simple web-based UI to see your broker's topics, consumers, etc.  
Access it in your browser at http://localhost:8080

To create these Docker containers, I've created a simple Powershell script:  

`PS ..\Kafka Playground\docker> .\StartKafka.ps1`

>NOTE: The Docker Volumes created by docker compose will NOT be automatically deleted if you remove the containers. They will be used to persist data between containers.

To delete these containers, you can use the following script:  

`PS ..\Kafka Playground\docker> .\StopKafka.ps1 -RemoveVolumes`  

>NOTE: By including the -RemoveVolumes flag, you will tell Docker to remove the Volumes that were created. If you do this, you will lose existing topics, messages and other data.

## Example1 ##
Example1 comes from https://developer.confluent.io/quickstart/kafka-docker/
This has two classes that each have a main() method, so they can be run independently:
* **ProducerExample** - Connects to a Kafka topic called "purchases" and generates 10 random messages
* **ConsumerExmaple** - Connects to a Kafka topic called "purchases" and reads messages as they come in
  * This app is meant to keep running while ProducerExample creates messages. 
  * You should see the messages appearing in real-time 
  * There is no clean way to close this app since it is just a test. This may cause weirdness in your Kafka broker later, but it still shows how Consumers work.
* When these run, they expect the name of a properties file passed to them as a command line option
  * You should pass "getting-started.properties" as the first parameter in the command line
  * In IntelliJ, you can configure run configurations to do this automatically

## TestingUI ##
This example includes a Producer, a Kafka Streams processor, and 2 Consumers
* **ProducerRunner**
  * This create a random message that is either a Person or a Transaction
  * These messages are encoded with ProtoBuf and sent to the "testing-input" topic
  * Each message that is read is output to a function given to the ProducerRunner instance (this allows it to write to the UI in an uncoupled manner)
* **StreamsProducer**
  * This creates a Kafka Stream that reads from the "testing-input" topic
  * The stream splits messages to three new Streams called "Branch-ids", "Branch-transactions", and "Branch-Unknown"
  * All Person messages (messages with keys named "ID_xxx") are sent to "Branch-ids" and to a topic called "ids"
  * Transaction messages (messages with keys named "Tr_xxx") are sent to "Branch-transactions"
  * Transactions with an amount field that is greater than $200 are sent to a topic called "transactions"
  * Any unknown messages (there shouldn't be any) are sent to "Branch-Unknown" and then to the "ids" topic
* **ConsumerRunner**
  * This reads records from a topic that is assigned in the constructor
  * The message is decoded from ProtoBuf into either a Person or a Transaction, depending on the message's key
  * Each message that is read is output to a function given to the ProducerRunner instance (this allows it to write to the UI in an uncoupled manner)

### Compiling Protobuf Messages ###
These classes all refer to classes that need to be generated by ProtoBuf. That is done by running the following:  

`PS ..\Kafka Playground\TestingUI> mvn protobuf:compile`  

(You can also run it from IntelliJ's Maven tool window and/or create a run configuration for it.)  
Once the Protobuf classes have been generated, the entire project should compile correctly.

### Running the application ###
To run this application, you need to use the JavaFX Maven plugin. This command will run the application:  

`PS ..\Kafka Playground\TestingUI> mvn javafx:run`  

(You can also use IntelliJ's Maven tool window or a run configuration to do this.)

### Putting it together ###
The easiest way to see this example in action is to start each Consumer, then start the Producer.  
Messages will be created by the Producer, and shown in its text area.  
The StreamsProducer will log the key of each message it sees, along with printing where it is sending the message.  
Person messages will be logged in the PersonConsumer's text area, while Transactions with values over $200 will be shown in that text area.